Notes for Lesson 1-Introduction:

    Pt. 1 Intro to ML:

        Deep Learning vs. Machine Learning:

            Machine Learning - A portfolio of statistical algorithms that look at your data and learn something to make a prediction.

            AI - The application that is built on top of the ML models. The results of a deployed model.

            Deep Learning - A type of more advanced ML. Not different just more specific.

        Introduction to Machine Learning - MIT's John Guttag

            Machine Learning - "Computers Learning without being explicitly programmed" - Arthur Samuel (1959)

            Traditional Programming:

                Data    ->
                            Computer -> Output
                Program ->
            
            Machine Learning:

                Data    ->
                            Computer -> Program
                Output  ->

            How are things learned

                Memorization:

                    Accumulation of individual facts

                    This is declarative knowledge

                Generalization:

                    Deduce new facts from old facts

                    This is imperative knowledge

                In ML we are seeking generalization

            Supervised Learning:

                Given a set of feature/label pairs, find a rule that predicts the label associated with a previously unseen input

            Unsupervised Learning:

                Given a set of feature vectors (without labels) group them into "natural clusters" (or create labels for groups)

            Feature Engineering:

                What are the features in my dataset and how do I weight those features to create the optimal model?

            Manhattan Distance:

                Given points (x,y) and (x1, y1)

                Manhattan Distance = abs(x-x1) - abs(y-y1)

                Calculating this distance you measure the distance of moving only parallel to each axis and not taking the actual
                shortest straight route between each destination

                This would be useful for determining distance in a discrete space

            Euclidean distance:

                Given points (x,y) and (x1, y1)

                Euclidean Distance = ((x-x1)^2 - (y-y1)^2)^(1/2)

                Calculating this distance measures the shortest distance by moving directly to the destination with no turns

                This is useful for determining distance in a continous space

            Accuracy:

                The predictive accuracy of a model

                accuracy = (True Positive + True Negative) / (True Positive + True Negative + False Positive + False Negative)

            Positive Predictive Value (PPV):

                The accuracy of a model for positive predictions

                PPV = True Positive / (True Positive + False Positive)

            Sensitivity:

                The percentage of positive samples in the dataset correctly identified

                sensitivity = True Positive / (True Positive + False Negative)

            Specificity:

                The percentage of negative samples in the dataset correctly rejected

                specificity = True Negative / (True Negative + False Positive)

    Pt. 2 History of ML:

        1950 Alan turing laid the foundation for machines that can think

        1956 Dartmouth Summer Research Project coins the terms Artificial Intelligence

        1956-1974 Researchers were confident in the future of the field and thought that it would progress rapidly

            Achievments in this period:

                First chat bot Eliza developed

                Shakey the robot

                Blocks world

        1974-1980 Confidence in the future of the field diminished due to overcoming problems such as computer power, exponential growth in required parameters
        paucity of data, the 'chinese room theory' brought into question the turing test, the ethics of introducing AI into society was questioned

            Paradigm forms between scruffy vs. neat AI. Scruffy AI had researchers develop algorithms and special case solutions for each problem to produce
            intelligent behaviour while neat AI tried to find a more general approach to developing intelligence that can be iterated and improved upon to achieve
            general intelligence or superintelligence. After the 90's neat became the almost exclusively used approach.

        1980s Expert Systems

            Businesses began to use the first successful forms of AI known as expert systems which utilized a rules engine for business requirement and an inference
            system that used rules to deduce new facts.

        1987-1993 AI Chill

            AI systems were less commonly used in this time period but the start of personal computing opened up the development of big data in the future

        1993-2011 Problems caused by lack of computing power and lack of data were starting to be overcome in this time period and it caused rapid development of
        the field again.

        Now AI is part of our everyday lives and the ethics of using AI is now a much larger issue that needs addressed

    Pt. 3 Fairness in ML:

        Introduction

            If you are building a model based off of data that lacks certain demographics, such as race, gender, political view, religion, or disproportionally represents
            those demographics then you will build a model with unfair biases

        Unfairness in data and algorithms

            If you manipulate data enough you can get it to produce any outcome you want. Sometimes this manipulation is unintentional due to implicit biases but
            it is important to be aware of this and work to remove these biases from our data. 

        Fairness-related harms

            Main Fainess related harms

                Allocation: Favoring one group over another

                Quality of Service: Data is trained for one specific scenario but in reality it is more complex leading to worse service

                Stereotyping: Associating a given group with pre-assigned attributes

                Denigration: Unfairly criticize and label something or someone

                Over- or under- representation: The idea that a certain group is not seen in a profession and a service or function that keeps promoting it
                makes this a self-fulfilling prophecy

        Detecting Unfairness

            Social biases, inadequate data and wrong assumptions can cause unfairness so it is important to watch out for them

            Assessment Methods

                1. Identify harms (and benefits): Think about how actions and decisions can affect both potential customers and a business itself

                2. Identify the affected groups: Once harms and benefits are found, identify the groups that may be affected

                3. Define fairness metrics: Define a metric so you have something to measure against and improve

            Use Fairlearn a python library for increasing fairness in models

    Pt. 4 Techniques of ML:

        High level steps for creating ML processes

            1. Decide on the question: ML processes usually start from a point of not being able to make a rules-based engine or conditional program that will
            solve the problem

            2. Collect and Prepare Data: This phase includes collecting data that will help you answer your question. Quality and quantity of data will likely
            decide how well you can answer that question. In this phase you will also work on visualizing the data and splitting it into training and testing groups

            3. Choose a training method: This is the part that requires expertise in order to provide the correct model with appropriate parameters to the dataset that
            you have. This phase often requires a lot of experimentation

            4. Train the Model: Use your training data to help the model recognize patterns in the data

            5. Evaluate the Model: Use never before seen data (testing data) to test your models performance

            6. Parameter Tuning: Based on your models performance you can redo the process using different parameters or variables that control the behavior of
            the algorithms used to train the model 

            7. Predict: Use new inputs to test the accuracy of your model

        Features and Target

            Feature

                A measurable property of your data

                Feature Extraction: Creates new features from functions of the original features

                Feature Selection: A subset of the whole feature set 

            Target

                The thing you are trying to predict with your data

        Visualizing your data

            Libraries like MatPlotLib and Seaborn can help you get a new perspective on your data through visualization.

        Training Model

            Model Fitting: Accuracy of a models underlying function as it attempts to analyze data it is not familiar with

            Overfit Model: The model is fit too specifically to the training data. Meaning it performs well on training data and poor on testing data
            
            Underfit: The model fails to find any clear pattern in the data and performs poorly on both testing and training data
            
             

